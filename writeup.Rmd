---
title: "Boost The Vote"
author: "David Marx"
date: "Friday, November 28, 2014"
output: html_document
---

DISCLAIMER: The following thought experiment is highly idealized, and only really makes sense in the abstract. If the policies I suggest below were actually implemented, contrary to having an objectively positive effect, it's almost certain that the suggested process would be corrupted to bring about whatever outcome the people in control of the processes wanted. Additionally, the idea I present is very likely morally objectable. 

# Motivation

A few weeks ago I was taking a shower when I had an idea about the US electoral process. There's been a lot of discussion about the role of money in politics, and it got me thinking about the kinds of tangible incentives that do and don't motivate politicians to varying degrees. This brought me to the question: is there a way we could construct a system such that bringing about positive outcomes is directly incentivized? 

# The Idea

There are people who are well informed and vote intelligently to bring about positive outcomes. There are people who are poorly informed and whose voting behavior could be modelled by a coin flip. And there are people who are mis-informed and who consistently vote in opposition to their own best interest. These different types of people are distributed in some way across the population. If we could somehow favor the votes of the more informed population over the ill or misinformed electorate, we could essentially assure positive outcomes. Additionally, politicians would have a tangible incentive to bring about positive outcomes, because to do otherwise would disenfranchise their electorate and make it much harder for them to be re-elected.

If we can increase the voting power of the "better" voters, we could potentially increase the probability of positive electoral outcomes.

# The model

Let's assume first that there is some objective way of measuring whether or not an election had a positive outcome. Maybe this would be a function of the status of the economy, or peace, or human wellfare. It really doesn't matter: this is a thought experiment. Just imagine that such an objective metric could feasibly constructed (it probably/almost certainly cannot). 

With such a metric defined, we could determine whether or not an individual citizen's vote contributed to a positive outcome or not. Let's further assume that an individual's voting behavior can be reasonably modeled by the probability that their vote will contribute to a positive outcome, as measured by our imaginary metric. 

Each voter can be thought of as a classification model that tries to classify an electoral candidate as worthy of their vote or not. The probability of a voter contributing to a positive outcome can be thought of as the model's accuracy. The election is then an ensemblificiation of our classifiers using a simple average to combine the respective classificaitons. Our goal is to learn a set of weights that will increase the accuracy of our ensemble by using a *weighted* average in place of a simple average. 

If we had access to each model's true (fixed) accuracy, we could optimize these weights deterministically. But since we don't, we'll have to use a stochastic approach.

To complicate matters further, our "models"" are ephemeral: old voters die and are replaced by young voters whose accuracy does not necessarily match the voter they are replacing. So we need to perpetually update our model weights. Therefore, an incremental approach is called for.

For simplicity, let's assume that the percentage of voter churn is fixed, and that new voters are generated from the same probability distribution as the voters they are replacing. 

# Election Simulation

Let's start by defining the variables on which our simulation will depend.

* **Population size** - We can adjust this all we want, but once it's large enough it shouldn't really have much if any effect on our simulations.

* **Voter accuracy distribution** - This will determine who the members of our population are. Each voter either contributes to a positive outcome or does not, so they're vote can be treated as bernoulli trial. The voter generates trial outcomes subject to some probability (which is how we will encode how informed a voter they are). A great way to model a distribution over a particular distribution's parameters is via its [conjugate distribution](https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions). In our case, the bernoulli/binomial distributions are conjugate to the [*Beta* distribution](https://en.wikipedia.org/wiki/Beta_distribution), which takes two "shape" parameters, $\alpha$ and $\beta$, so we'll enumerate these as inputs to the simulation.

* **Learning rate** how much we allow our model (i.e. the collection of voter weights) to change during each iteration. The effect of the learning rate is going to be balanced by the "population churn rate"" described below.

* **Population churn rate** Essentially a combined birth/mortality rate for our voting population, which for simplicity purposes we assume to be at a population equilibrium. This will affect how long we are able to learn the weights for an individual (i.e. before they "die" in the simulation), which in turn affects how close a given individual's weight can get to convergence. Higher churn means individual's will exit the model farther from convergence, so slow learning rates will be less effective.

* **Number of elections** - How many sequential elections will we simulate? This is equivalent to the number of simulation iterations/generations.

Let's start with a population of 1000, a 10% churn rate, a slow learning rate, 10 elections, and a distribution of voters that puts most people in the middle with a few really intelligent voters and a few really terrible voters:

```{r}
pop_size = 1e3
a=2; b=2 # on average, people vote no better than a coin flip.
boosting_factor = .2 # penalty/bonus for predicting good outcomes
elections = 10 # number of elections
churn_rate=.1 # births/deaths voter churn
```

Here's what the voter distribution looks like:

```{r}
voters = rbeta(pop_size, a, b)
plot(density(voters), lty=2, col='blue', xlim=c(0,1))

xv= seq(0,1,.01)
lines(xv, dbeta(xv, a,b))
```

Now, to simulate a single election, we let the voters each cast a vote based on their individual voting probabilities. Their voteeitehr contributes to a positive outcome or not relative to their voting probabilty, so we can simulate the election by drawing a random bernoulli draw for each voter:

```{r}
# Voting results for a single election
outcomes = rbinom(pop_size, 1, voters)
mean(outcomes)
```

As mentioned above, our goal is to iteratively learn weights to construct a weighted average over our voting population that boosts the result of the election towards a positive outcome (i.e. a result greater than $0.5$). 


```{r}
boost_the_vote = function(
  pop_size = 1e3,
  a=2, b=2, # on average, people vote no better than a coin flips.
  boosting_factor = .2, # penalty/bonus for predicting good outcomes
  elections = 10, # number of elections
  churn_rate=.1 # births/deaths voter churn
){
  # Let's simulate "boosting the vote"
  #
  # ~~VOTING POPULATION~~
  voters = rbeta(pop_size, a, b)
  # initialize all voters as having equal contributions to the process
  voting_power = rep(1, pop_size)
  # define a linear penalty/bonus for making a good/bad decision
  penalty = 1 - boosting_factor
  bonus = 1 + boosting_factor
  # after each election, update everyones voting power relative to whether or not they made the right choice
  result = rep(NA, elections)
  voter_age = rep(0, pop_size) # Track the number of elections a voter has participated in
  n_churn = pop_size * churn_rate
  for(i in 1:elections){
    # Run the election
    outcomes = rbinom(pop_size, 1, voters)
    result[i] = sum(voting_power*outcomes)/sum(voting_power)
    # weighted average
    # Update voting power based on results
    voting_power[outcomes==1] = voting_power[outcomes==1] * bonus
    voting_power[outcomes==0] = voting_power[outcomes==0] * penalty
    
    # Kill of oldest voters, replace with fresh voters
    #ix = order(voter_age, ascending=False)[1:n_churn]
    voter_age = voter_age + 1
    ix = sample(pop_size, n_churn, prob = voter_age/sum(voter_age))
    voters[ix] = rbeta(n_churn, a, b)
    voting_power[ix] = 1
  }
  list(voters=voters, voting_power=voting_power,outcomes=result)
}
```

```{r}
elections_sensitivity_plot = function(k = 100,
                                      boosting_factors = c(0, 0.2,
                                                           0.5, 0.8), 
                                      #q = c(0.2, 0.5, 0.8), #60% CI
                                      #q = c(0.05, 0.5, 0.95) # 90% CI
                                      conf_intv = .9,
                                      n = 30, 
                                      a = 1, 
                                      b = 1) {
  q_delta = (1-conf_intv)/2
  q = c(q_delta, .5, 1-q_delta)
  votes = lapply(boosting_factors, function(x) {
    v = replicate(k, 
                  boost_the_vote(boosting_factor = x,
                                 elections = n,
                                 a = a, b = b)$outcomes
    )
    apply(v, 1, function(row) {
      quantile(row, q)
    })
  })
  plot_title = paste("Mortality Model: Beta(a=",a,", b=",b,")", sep="")
  plot(0, 0, xlim = c(0, n), ylim = c(0, 1), xlab = "Elections", ylab = "Outcome", main=plot_title)
  for (i in 1:length(votes)) {
    lines(1:n, votes[[i]][1, ], col = i, lty = 2)
    lines(1:n, votes[[i]][2, ], col = i, lty = 1)
    lines(1:n, votes[[i]][3, ], col = i, lty = 2)
  }
}
```


elections_sensitivity_plot(conf_intv=.8)

```{r}
xv = seq(0,1,.01)
plot(xv, dbeta(xv, 2,5), type='l')
elections_sensitivity_plot(k=100, a=2, b=2, conf_intv=.7)
```
