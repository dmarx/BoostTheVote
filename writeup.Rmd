---
title: "Boost The Vote"
author: "David Marx"
date: "Friday, November 28, 2014"
output: html_document
---

DISCLAIMER: The following thought experiment is highly idealized, and only really makes sense in the abstract. If the policies I suggest below were actually implemented, contrary to having an objectively positive effect, it's almost certain that the suggested process would be corrupted to bring about whatever outcome the people in control of the processes wanted. Additionally, the idea I present is very likely morally objectable. 

# Motivation

A few weeks ago I was taking a shower when I had an idea about the electoral process in the united states. There's been a lot of discussion about the role of money in politics, and it got me thinking about the kinds of tangible incentives that do and don't motivate politicians to varying degrees. This brought me to the question: is there a way we could construct a system such that bringing about positive outcomes is directly incentivized? 

# The Idea

There are people who are well informed and vote intelligently to bring about positive outcomes. There are people who are poorly informed and whose voting behavior could be modelled by a coin flip. And there are people who are mis-informed and who consistently vote in opposition to their best interest. These different types of people are distributed in some way across the population. If we could somehow favor the votes of the more informed population over the ill or misinformed electorate, we could essentially assure positive outcomes. Additionally, politicians would have a tangible incentive to bring about positive outcomes, because to do otherwise would disenfranchise their electorate and make it much harder for them to be re-elected.

Can we learn a model that increases the probability of good outcomes by increasing the voting power of certain voters?

# The model

Let's assume first that there is some objective way of measuring whether or not an election had a positive outcome. Maybe this would be a function of the status of the economy, or peace, or human wellfare. It really doesn't matter: just imagine that such an objective metric could feasibly constructed (it probablyalmost certainly cannot). WIth such a metric defined, we could determine whether or not individual citizen's votes contributed to a positive outcome or not. Let's further assume that an individual's voting behavior can be reasonably modeled by the probability that their vote will contribute to a positive outcome, as measured by our imaginary metric. 

Each voter can be thought of as a classification model, and the probability of a voter contributing to a positive outcome can be thought of as the model accuracy. The election is then an ensemblificiation of our classifiers using a simple average to combine the respective classificaitons. The hypothesis asserted in the previous section is that there is a way to learn a set of weights that will increase the accuracy of our ensemble by using a *weighted* average in place of a simple average. 

If we had access to each model's true (fixed) accuracy, we could optimize these weights deterministically. But since we don't, we'll have to use a stochastic approach.

To complicate matters further, our models are transitive: old voters die and are replaced by young voters whose accuracy does not necessarily match the voter they are replacing. So we need to perpetually update out model weights. Therefore, an incremental approach is called for. 

For simplicity, let's assume that the percentage of voter churn is fixed, and that new voters are generated from the same probability distribution as the voters they are replacing. 

# Simulation

Let's start by defining the variables on which our simulation will depend.

* **Population size** - We can adjust this all we want, but once it's large enough it shouldn't really have much if any effect on our simulations. In any event, it's a parameter and so should be attached to a variable instead of being fixed. It's generally a good practice to avoid hardcoding "magic numbers" into your simulations.
* **Voter accuracy distribution** - This will determine who the members of our population are. A good distribution for modeling distributions of probabilities is the *Beta* distribution, so that's what I'll be using. This distribution takes two "shape" parameters, $\alpha$ and $\beta$, so we'll include these as separate variables in our model.
* **Population churn rate** Essentially a simplified mortality rate for our voting population. Will affect how long we have to learn the weights for an individual, so more churn means slower learning rates will produce much less valuable models.
* **Learning rate** how much we allow our model to change during each iteration. The effect of the learning rate is going to be balanced by the population churn as described above.
* **Number of elections** - How many sequential elections will we simulate?

Let's start with a population of 1000, a 10% mortality rate, a slow learning rate, 10 elections, and a distribution of voters that puts most people in the middle with a few really intelligent voters and a few really terrible voters:

```{r}
pop_size = 1e3
a=2; b=2 # on average, people vote no better than a coin flip.
boosting_factor = .2 # penalty/bonus for predicting good outcomes
elections = 10 # number of elections
churn_rate=.1 # births/deaths voter churn
```

Here's what the voter distribution looks like:

```{r}
voters = rbeta(pop_size, a, b)
plot(density(voters), lty=2, col='blue', xlim=c(0,1))

xv= seq(0,1,.01)
lines(xv, dbeta(xv, a,b))
```

Now, to simulate a single election, we let the voters each cast a vote based on their individual voting probabilities. Their voteeitehr contributes to a positive outcome or not relative to their voting probabilty, so we can simulate the election by drawing a random bernoulli draw for each voter:

```{r}
# Voting results for a single election
outcomes = rbinom(pop_size, 1, voters)
mean(outcomes)
```

As mentioned above, our goal is to iteratively learn weights to construct a weighted average over our voting population that boosts the result of the election towards a positive outcome (i.e. a result greater than $0.5$). 


```{r}
boost_the_vote = function(
  pop_size = 1e3,
  a=2, b=2, # on average, people vote no better than a coin flips.
  boosting_factor = .2, # penalty/bonus for predicting good outcomes
  elections = 10, # number of elections
  churn_rate=.1 # births/deaths voter churn
){
  # Let's simulate "boosting the vote"
  #
  # ~~VOTING POPULATION~~
  voters = rbeta(pop_size, a, b)
  # initialize all voters as having equal contributions to the process
  voting_power = rep(1, pop_size)
  # define a linear penalty/bonus for making a good/bad decision
  penalty = 1 - boosting_factor
  bonus = 1 + boosting_factor
  # after each election, update everyones voting power relative to whether or not they made the right choice
  result = rep(NA, elections)
  voter_age = rep(0, pop_size) # Track the number of elections a voter has participated in
  n_churn = pop_size * churn_rate
  for(i in 1:elections){
    # Run the election
    outcomes = rbinom(pop_size, 1, voters)
    result[i] = sum(voting_power*outcomes)/sum(voting_power)
    # weighted average
    # Update voting power based on results
    voting_power[outcomes==1] = voting_power[outcomes==1] * bonus
    voting_power[outcomes==0] = voting_power[outcomes==0] * penalty
    
    # Kill of oldest voters, replace with fresh voters
    #ix = order(voter_age, ascending=False)[1:n_churn]
    voter_age = voter_age + 1
    ix = sample(pop_size, n_churn, prob = voter_age/sum(voter_age))
    voters[ix] = rbeta(n_churn, a, b)
    voting_power[ix] = 1
  }
  list(voters=voters, voting_power=voting_power,outcomes=result)
}
```

```{r}
elections_sensitivity_plot = function(k = 100,
                                      boosting_factors = c(0, 0.2,
                                                           0.5, 0.8), 
                                      #q = c(0.2, 0.5, 0.8), #60% CI
                                      #q = c(0.05, 0.5, 0.95) # 90% CI
                                      conf_intv = .9,
                                      n = 30, 
                                      a = 1, 
                                      b = 1) {
  q_delta = (1-conf_intv)/2
  q = c(q_delta, .5, 1-q_delta)
  votes = lapply(boosting_factors, function(x) {
    v = replicate(k, 
                  boost_the_vote(boosting_factor = x,
                                 elections = n,
                                 a = a, b = b)$outcomes
    )
    apply(v, 1, function(row) {
      quantile(row, q)
    })
  })
  plot_title = paste("Mortality Model: Beta(a=",a,", b=",b,")", sep="")
  plot(0, 0, xlim = c(0, n), ylim = c(0, 1), xlab = "Elections", ylab = "Outcome", main=plot_title)
  for (i in 1:length(votes)) {
    lines(1:n, votes[[i]][1, ], col = i, lty = 2)
    lines(1:n, votes[[i]][2, ], col = i, lty = 1)
    lines(1:n, votes[[i]][3, ], col = i, lty = 2)
  }
}
```


elections_sensitivity_plot(conf_intv=.8)

```{r}
xv = seq(0,1,.01)
plot(xv, dbeta(xv, 2,5), type='l')
elections_sensitivity_plot(k=100, a=2, b=2, conf_intv=.7)
```
